use std::cmp::Ordering;
use std::ops::Deref;

use crate::evaluator::*;
use crate::game_state::*;

/// The trait for strategies. Given a DynamicGameState, return either an Action or a GameError.
/// Strategies can use the output of the Evaluator in very different ways. For instance, you may
/// have an evaluator that returns a policy. From here, what should you do with the given probabilities?
/// You could simply pick the action with the highest probability, or you could do a Monte-Carlo
/// tree-search type algorithm where you randomly sample from the given probabilities and
/// potentially recurse through the Game's states with the sampled actions.
///
/// Generally, provided strategies assume that the game is not over. This avoids a redundant call
/// to DynamicGameState::outcome() while inside best_action(). The provided GamePlayers all call
/// this function before calling best_action(), and thus the code in this module reflects that.
pub trait Strategy<G, E>
where
    G: GameState,
    E: Evaluator<G>,
{
    fn choose(&mut self, state: &G, evaluator: &mut E) -> G::Action;
}

/// For some games, it is natural to return a reference to an action that was generated by
/// legal_actions(). In this case there is no point in cloning the action, as it is usually simply
/// consumed by apply() and dropped thereafter; however, for some games it is not feasible to
/// generate all legal actions. For instance, if your action picks a value in a range or if your
/// action is a String that is dynamically generated and not picked from a predefined list. In
/// these cases, the strategy may have to generate the action from within the function call and
/// therefore must return an owned value.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum ActionRef<'a, A> {
    Owned(A),
    Borrowed(&'a A),
}

impl<'a, A> Deref for ActionRef<'a, A> {
    type Target = A;

    fn deref(&self) -> &Self::Target {
        match self {
            ActionRef::Owned(a) => a,
            ActionRef::Borrowed(a) => *a,
        }
    }
}

/// Takes an Evaluator whose Evaluation impl's PartialOrd and returns the action that has the highest
/// Evaluation. This strategy is optimal for two player finite games w perfect information; the
/// only catch is computing the evaluations ;).
pub struct GreedyStrategy;

impl<G, E> Strategy<G, E> for GreedyStrategy
where
    G: GameState,
    G::Action: Clone,
    E: Evaluator<G>,
    E::Evaluation: PartialOrd,
{
    fn choose(&mut self, state: &G, evaluator: &mut E) -> G::Action {
        let mut actions = state.legal_actions();
        let mut best_action = actions
            .next()
            .expect("Game isn't over but there were no legal moves available.");
        let mut best_eval = evaluator.evaluate(state, best_action);
        for action in actions {
            let eval = evaluator.evaluate(state, action);
            match best_eval.partial_cmp(&eval) {
                Some(Ordering::Less) => {
                    best_action = action;
                    best_eval = eval;
                }
                Some(_) => (),
                None => panic!("Evaluator returned an evaluation that couldn't be compared"),
            };
        }
        best_action.clone()
    }
}
